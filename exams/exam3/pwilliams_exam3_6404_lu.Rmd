---
title: |
  | \vspace{3cm} \LARGE{Exam 3: Wavelets, Categorical Data Analysis, and Nonparametric Regression}
author:
- Peter Williams, pwilliams60@gatech.edu

date: "`r paste0('Date: ',Sys.Date())`"
output:
   pdf_document:
      fig_caption: true
      number_sections: false
fontfamily: mathpazo
fontsize: 10pt
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{flushleft}}
  - \posttitle{\end{flushleft}}
  - \preauthor{\begin{flushleft}}
  - \postauthor{\end{flushleft}}
  - \predate{\begin{flushleft}}
  - \postdate{\end{flushleft}}
  - \usepackage{caption} 
  - \captionsetup[table]{skip=10pt}
---

\newpage
\tableofcontents

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(np))
suppressPackageStartupMessages(library(parallel))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(waveslim))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(xtable))
```

## 1. Wavelets (50%)

*Locate a one-dimensional data set that has sharp-changes like those presented in the recent lectures for applying the following wavelet procedures. It is best that the data size is larger than 512, and is in 2-factorial, e.g., 210 = 1024.  Note that you need to locate proper R-package/codes to perform the tasks below.*  

For these exercises a dataset was located from the data science competition website, Kaggle (https://www.kaggle.com/datasets). The dataset consists of estimated energy consumption in Megawatts (MW) by hour provided by American Electric Power (AEP). AEP is a major investor-owned electric utility in the United States of America, which provides electricity to more than five million customers in 11 states including Texas and Ohio. 

To simplify subsequent wavelet analysis, a subset of the original dataset was utilized. It consists of $n = 1024$ observations of hourly Megawatt consumption estimates beginning from 9:00am June 21st, 2018 to 12:00am August 3rd, 2018. A preview of the original dataset is shown below:  

```{r wave_dat, include = T, echo = F}
aedat <- fread('/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/AEP_hourly.csv', 
                  header = T, stringsAsFactors = F)
aep <- aedat[(nrow(aedat) - 1024 + 1):nrow(aedat)]
knitr::kable(head(aep), caption = 'AEP Hourly Energy Consumption in Megawatts', col.names = c('Hourly  (Datetime)', 'Energy Usage (MW)'),format.args = list(big.mark = ','))
```

*i)	Select two families of wavelets for completing the two tasks ii) and iii) below, and make comparisons for the results impacted from distinct wavelet families.*

*ii)	Show a multi-resolution plot of mother and father discrete wavelet-coefficients (DWTs), and make comments about their values.*

A multi-resolution plot, including the original time series of energy consumption data, provides a lot of context on the data. The plot of the original data shown below highlights significant shifts in energy consumption in the time series, and a persistent daily cyclical pattern. 

To contrast the multi-resolution plots below, we have selected both the 'Haar' and 'Daubechies' filters for comparison in the columns of the figure below. For visual simplicity and interpretation both plots rely on 4 levels, or depth of signal decomposition. The MRA plot quickly highlights, for both families, that for the first scaling level $S2$, we have a good representation of the original function, and that at further depth the signals are much noisier. Also worth noting is the contrast in smoothness of the 'Daubechies' first level decomposition vs. the 'Haar'. In fact the first level for the 'Daubechies' family captures more of the original signal, as the magnitude, or scale of coefficients at lower levels is tighter relative to the 'Haar' decomposition, as shown below: 

```{r mra_plots_haar, include = T, echo = F, fig.align='center', fig.height=8}
x <- as.numeric(aep$AEP_MW) #input megawatts time series
#mra.out <- mra(X =  x, n.levels = 4, filter = 'haar', method = 'dwt')

n.levels <- 4
aep.haar <- mra(x = x, "haar", n.levels, "dwt")
#names(aep.haar) <- c("d1", "d2", "d3", "d4", "s4")
nplots <- n.levels + 2
par(mfcol=c(nplots,2), pty="m", mar=c(5-2.5,4,4-2,1), cex.main = 0.9, cex.axis=0.8)
plot.ts(x, axes=F, main="Haar Filter: Multi-Resolution Plot", ylab = 'Original', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(x),digits = -4),round(max(x),digits = -4)), 
     labels=c(round(min(x),digits = -4),round(max(x),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(aep.haar[[i]], axes=F, ylab=names(aep.haar)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(aep.haar[[i]]),digits = -2),round(max(aep.haar[[i]]),digits = -2)), 
       labels=c(round(min(aep.haar[[i]]),digits = -2),round(max(aep.haar[[i]]),digits = -2)))
}

aep.haar <- mra(x = x, "la8", n.levels, "dwt")
#names(aep.haar) <- c("d1", "d2", "d3", "d4", "s4")
nplots <- n.levels + 2
#par(mfcol=c(nplots,1), pty="m", mar=c(5-3.5,4,4-2,1.5))
plot.ts(x, axes=F, main="Daubechies Filter (L=8): Multi-Resolution Plot", ylab = 'Original', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(x),digits = -4),round(max(x),digits = -4)), 
     labels=c(round(min(x),digits = -4),round(max(x),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(aep.haar[[i]], axes=F, ylab=names(aep.haar)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
       labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(aep.haar[[i]]),digits = -2),round(max(aep.haar[[i]]),digits = -2)), 
       labels=c(round(min(aep.haar[[i]]),digits = -2),round(max(aep.haar[[i]]),digits = -2)))
}
```

*Note: To generate these plots, the 'waveslim' package was utilized in $R$, using the "mra" function with method "DWT" supplied as an argument which provides an additive decomposition of the original time series. Futher details are provided in the appendix.*  

\newpage

*iii)	Apply two thresholding/shrinkage methods to reduce number of non-zero DWT coefficients. Apply the IDWT to reconstruct the original data signal by using the thresholded DWTs. Comment on the quality of the reconstructions from plotting the original and reconstructed signals (by overlaying them in one figure). Compare the two methods about their data-reduction ratios and the MSEs.* 

To reduce the number of non-zero DWT coefficients, we rely on two popular shrinkage methodologies to find smooth representations of our original series. We utilize the *SURE* and *VisuShrink* thresholding methodologies, described by Donoho and Johnstone in their 1995 pager *Adapting to Unknown Smoothness via Wavelet Shrinkage*. We apply thresholding on our DWT that utilized 4 levels, and the Daubechies Filter described above. A very brief of these thresholding methodologies is provided here: 

The *VisuShrink* approach to thresholding relies on an estimate of the noise level of wavelet coefficients denoted $\hat{\sigma}$. $\hat{\sigma}$ is often implemented as a scaled median absolute deviation of the estimated wavelet coefficients. The thresholding rule applied relies on the estimate, *$T_{visu} = \sqrt{2(log\ n)\hat{\sigma}}$, where $n$ is the number of data points, and $\hat{\sigma}$ is as described above.   

The *SURE* methodology utilizes Stein's unbiased risk estimation, as described by by Donoho and Johnstone in their paper. This methodology creates a threshold rule $t_j$ for each resolution level $j$ in a wavelet transform, and is found using cross-validation.  

To constrast these two thresholding approaches, we plot the original signal in the figures below, against the reconstructed signal post-thresholding out DWT: 

```{r thresh_comps, include = T, echo = F, fig.align='center',fig.height=6}
#thresholding https://cran.r-project.org/web/packages/waveslim/waveslim.pdf
wv <- dwt(x, wf="la8", n.levels=4, boundary="periodic")
sure <- sure.thresh(wv, max.level = 3, hard =TRUE)
visu <- universal.thresh(wv, max.level = 3, hard = TRUE)
par(mfrow = c(2,1), cex.lab = 0.8)

plot(1:length(x), x, type = 'l', lwd = 4.5, bty = 'n', col = 'gray', ylim = c(8000,23000), 
     main = 'Wavelet Reconstruction: SURE Threshold', cex.main= 0.8, xlab = 'Hour',ylab='Energy Consumption')
lines(1:length(x), idwt(sure), col = 'navy', lty = 1, lwd = 1.4)
legend(80, 12000, c('Original Signal','Wavelet Reconstruction'), 
       col = c('gray','navy'), lty = 1, lwd = c(4.5,2.2), bty = 'n', cex = 0.6)

plot(1:length(x), x, type = 'l', lwd = 4.5, bty = 'n', col = 'gray',ylim = c(8000,23000), 
     main = 'Wavelet Reconstruction: VisuShrink Threshold', cex.main = 0.8,xlab = 'Hour',ylab='Energy Consumption')
lines(1:length(x), idwt(visu), col = 'navy', lty = 1, lwd = 1.4)
#idwt(sure)
```

Visually it is quite clear the *VisuShrink* thresholding approach provides a more representative reconstruction of the original signal. The *SURE* approach is clearly less smooth, but has a slightly higher data reduction ratio which is shown in summary in the table below. We also provide summaries of the mean squared error (MSE), the root mean squared error (RMSE, which can be more easily interpreted on the original data scale), along with the data reduction ratio (%), which is computed as $100 \cdot (1 - \frac{\#\text{non-zero DWT Coefficients} }{\# \text{Original Observations}})$:  

```{r sample, echo=FALSE, results='asis'}
#thresholding https://cran.r-project.org/web/packages/waveslim/waveslim.pdf
#data reduction ratios
dr_sum <- data.frame(
  threshold = c('SURE','VisuShrink'),
  ratio = 100*round(c(1 - (sum(unlist(lapply(sure, function(x) sum(x != 0))))/1024),
    (1-sum(unlist(lapply(visu, function(x) sum(x != 0))))/1024)),digits = 4)
)

#mses
mse_sum <- data.frame(
  threshold = c('SURE','VisuShrink'),
  mse = round(c(mean((x - idwt(sure))^2),
mean((x - idwt(visu))^2)),digits = 1), stringsAsFactors =F)

rmse_sum <- data.frame(
  threshold = c('SURE','VisuShrink'),
  rmse = round(c(sqrt(mean((x - idwt(sure))^2)),
sqrt(mean((x - idwt(visu))^2))),digits = 1), stringsAsFactors =F)

t1 <- kable(mse_sum, format = "latex",booktabs = TRUE)
t2 <- kable(rmse_sum, format = "latex",booktabs = TRUE)
t3 <- kable(dr_sum, format = "latex" ,booktabs = TRUE, col.names = c('threshold','ratio (%)'))

cat(c("\\begin{table}[!htb]
    \\begin{minipage}{.3\\linewidth}
      \\caption{MSE Results}
      \\centering",
        t1,
    "\\end{minipage}%
    \\begin{minipage}{.33\\linewidth}
      \\centering
        \\caption{RMSE Results}",
        t2,
        "\\end{minipage}%
    \\begin{minipage}{.3\\linewidth}
      \\centering
        \\caption{Reduction Ratio}",
        t3,
    "\\end{minipage} 
\\end{table}"
)) 
```

  
*MSE*, *RMSE* results provide further confirmation of the *VisuShrink* thresholding method's ability to provide a more accurate reconstruction of the original signal. Despite having a slightly lower reduction ratio, *VisuShrink* provides this improved reconstruction without a significant loss, with only a $\sim 7$ percent difference vs. the *SURE* methodology. 

*iv)	Artificially alter the data in one “local-region” and one large-size “global region” for creating 3 distinct “fault-class” data sets. See lecture presentation about details of this task.  Apply the best thresholding method (and the best wavelet-family) to model the data from all FOUR classes (one from the original data and the other 3 fault-class data). Use the multi-resolution plots of thresholded-DWTs to see how they are different in these FOUR classes of data signals.*  

To create our fault class datasets, we replicated our original data and added some artificial alterations. As shown in the figure below for 'Case 1', we added random noise to the original signal, specifically $N(0,\sigma^2)$, with $\sigma = 2000$, given the scale of our original data. For 'Case 2', we shifted our original series by 25 time units to the right, added a small amount of noise $\sigma = 10$, and appended a sequence of values equal to the sample mean to the front of the series. Finally for 'Case 3', we added a small 'U' shaped curve from observations $375-436$, and added a small amount of noise. Comparisons of our altered series and the original series are shown below: 

```{r create_fc, include = T, echo = F, fig.align = 'center',fig.height=4}
f1 <-  x + rnorm(2^10, sd = 2000) #noisy original signal
f2 <-  c(rep(mean(x), 30), x[31:1024]) + rnorm(2^10, sd = 10) #shift right 25 positions add sigma 1 - global change
f3 <- c(x[1:374],x[375] + seq(-15,15,by=0.5)^2,x[436:1024]) + rnorm(2^10, sd = 10)
par(mfrow = c(2,2), cex.main = 0.85)
plot(f1, type = 'l', bty = 'n', main = 'Case 1', xlab = 'Time', ylab = 'Energy') 
plot(f2, type = 'l', bty = 'n', main = 'Case 2', xlab = 'Time', ylab = 'Energy') 
plot(f3, type = 'l', bty = 'n', main = 'Case 3', xlab = 'Time', ylab = 'Energy') 
plot(x, type = 'l', bty = 'n', main = 'Original Signal', xlab = 'Time', ylab = 'Energy') 
```

To explore how these artificial faults classes differ in their wavelet decomposition we can look at their respective multi-resolution plots, all using 2 resolution levels to simplify and condense visualizations: 

```{r fault_mra1, include = T, echo = F, fig.align='center',fig.height=8}
n.levels <- 1

#names(aep.haar) <- c("d1", "d2", "d3", "d4", "s4")
nplots <- n.levels + 2
par(mfcol=c(nplots*2,2), pty="m", mar=c(5-2.5,4,4-2,1), cex.main = 0.9, cex.axis=0.8)

f1.dau <- mra(x = f1, "la8", n.levels, "dwt")
plot.ts(f1, axes=F, main="Fault Class 1: Multi-Resolution Plot", ylab = 'F1', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(f1),digits = -4),round(max(f1),digits = -4)), 
     labels=c(round(min(f1),digits = -4),round(max(f1),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(f1.dau[[i]], axes=F, ylab=names(f1.dau)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(f1.dau[[i]]),digits = -2),round(max(f1.dau[[i]]),digits = -2)), 
       labels=c(round(min(f1.dau[[i]]),digits = -2),round(max(f1.dau[[i]]),digits = -2)))
}

f3.dau <- mra(x = f3, "la8", n.levels, "dwt")
plot.ts(f3, axes=F, main="Fault Class 3: Multi-Resolution Plot", ylab = 'F3', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(f3),digits = -4),round(max(f3),digits = -4)), 
     labels=c(round(min(f3),digits = -4),round(max(f3),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(f3.dau[[i]], axes=F, ylab=names(f3.dau)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(f3.dau[[i]]),digits = -2),round(max(f3.dau[[i]]),digits = -2)), 
       labels=c(round(min(f3.dau[[i]]),digits = -2),round(max(f3.dau[[i]]),digits = -2)))
}

f2.dau <- mra(x = f2, "la8", n.levels, "dwt")
plot.ts(f2, axes=F, main="Fault Class 2: Multi-Resolution Plot", ylab = 'F2', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(f2),digits = -4),round(max(f2),digits = -4)), 
     labels=c(round(min(f2),digits = -4),round(max(f2),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(f2.dau[[i]], axes=F, ylab=names(f2.dau)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(f2.dau[[i]]),digits = -2),round(max(f2.dau[[i]]),digits = -2)), 
       labels=c(round(min(f2.dau[[i]]),digits = -2),round(max(f2.dau[[i]]),digits = -2)))
}

aep.la8 <- mra(x = x, "la8", n.levels, "dwt")
#par(mfcol=c(nplots,1), pty="m", mar=c(5-3.5,4,4-2,1.5))
plot.ts(x, axes=F, main="Original Signal: Multi-Resolution Plot", ylab = 'Original', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(x),digits = -4),round(max(x),digits = -4)), 
     labels=c(round(min(x),digits = -4),round(max(x),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(aep.la8[[i]], axes=F, ylab=names(aep.la8)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
       labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(aep.la8[[i]]),digits = -2),round(max(aep.la8[[i]]),digits = -2)), 
       labels=c(round(min(aep.la8[[i]]),digits = -2),round(max(aep.la8[[i]]),digits = -2)))
}
```
   
*Note: DWT decomposition performed using 'Daubechies' filter.*  

The multi-resolution plots above provide visual indications of differences across fault classes. For example, at the 'D1' resolution level, both the shifted fault class, 'Fault Class 2', and the artificially altered series 'Fault Class 3' show significant spikes in coefficient estimate level around the area of the signal where the series was altered. 'Fault Class' which was altered with a larger magnitude of noise, has significantly larger estimates across the entire signal for wavelet coefficients at level 'D1'. These visual difference highlight a possibility of classifying fault classes based on features that can be extracted from values of the wavelet coefficient estimates are various levels. This topic will be explored further in the next section. 

*v)	Discuss the possibility and steps for developing a rigorous decision-making procedure to detect faulty-signals against the original data, and distinguish classes of faulty signals with reduced-size data presented by thresholded DWTs.*  

In the paper *Wavelet-Based Data Reduction Techniques for Process Fault Detection* (2006), Jeong and Lu et al. provide in-depth discussion on decision-making procedures for distinguishing classes of faulty signals with reduced size dataset. Based on the discussion in that and paper and our results from above, a decision-making procedure to detect and distinguish classes of faulty-signals should leverage: 

* A structured data generation process to support a diverse, and robust representation of fault-type classifications,
    + In order to detect and distinguish classes of faulty signal, data should be generated that form a diverse set of classes representative of what faulty signals may look like
    + A variety of techniques of generating faulty-classes are described by Jeong and Lu et al. include adding random noise existing datasets, adding artificial features and curves, and shifting time series etc.

* Features of the original data that are estimated from the multi-resolution decomposition, for example, 
    + Coarse level wavelet coefficients can be used to identify faults that relate to global faults related to the shape of the original data series relative to new signals
    + Granular wavelet coefficients, represented at lower levels of the decomposition can be used to identify faults that related local and global faults representing faults smaller in magnitude relative to the dominant signal patterns
  
To undertake these tasks described programmatically, Jeong and Lu et al. utilize a classification and regression tree (CART) to classify datasets into fault classes by relying on estimates of wavelet coefficients at different resolution levels in the signal decomposition of both faulty, and original signals. This approach progammatically incorporates some of the visual indications of differences across fault classes that are highlighted in the multi-resolution plots provided in the previous section. By incorporating resolution level, and coefficient magnitude as features in a decision tree, with the fault class as the outcome of interest, a decision tree can yield insight and mechanisms to predict fault classes given values from a wavelet decomposition directly. 

## 2. Categorical Data Analysis (25%)

*Chapter 9 of the textbook on categorical data analysis includes 6 sections with various problems/data and methods. Locate three sets of problems/data matching three distinct methods taught in lectures. Apply proper statistical software (preferred to be in R-codes) to analyze the data. Provide in-depth comments about the findings in your statistical analyses.*

*1. Chi-square, goodness of fit*

To discuss the Chi-square goodness of fit procedure, a dataset published by the *National Oceanic and Atmospheric Administration* was located that tracks the number of hurricanes that have made landfall on the continental United States by decade over the last $\sim 60$ years. Links to the actual source data is linked below. To get a sense of what the dataset looks like a brief preview is shown below, where the expected count of hurricanes is computed as the grand mean of observed landfalls across the entire dataset:      

```{r ch_gof, include = T, echo =F}
hdat <- read.table(file = '/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/hurricane-frequencies.txt',header=T,stringsAsFactors=F)
hdat <- transform(hdat, Expected_Count = round(sum(Count)/nrow(hdat),digits = 3))
T_stat <- round(sum((hdat$Count - hdat$Expected_Count)^2 / hdat$Expected_Count),digits = 3)
p_value <- round(1 - pchisq(T_stat, df = nrow(hdat)-1),digits=3)
knitr::kable(hdat, caption = 'NOAA: Continental United States: Hurricane Impacts/Landfalls', col.names = c('Decade','Count of Impacts','Expected Impacts'))
```
   
*Source: Continental United States Hurricane Impacts/Landfalls by decade as reported by the NOAA http://www.aoml.noaa.gov/hrd/hurdat/All_U.S._Hurricanes.html*

Given the overall national interest in climate change, and its impact on weather patterns, it is of interest to many researchers if the frequency of hurricane landfalls in recent years in increasing. Given the dataset shown above, we can test this hypothesis in a crude way utilizing differences in observed vs. expected frequency of hurricane landfall. Where $n_i$ refers to observed counts across $i = 1,...,k$ decades measured, and $N = \sum_{=1}^{k} n_i$, is the total number of observed landfalls. We can then set up our test:
$$H_0: F_X(x) = F_0(x),\ H_{\alpha}: F_X(x) \neq F_0(x)$$       

We then take our test statistic $T$ to be, $T = \sum_{i}^{r} \frac{(n_i - np_i)^2}{np_i}$, which is approximately distributed $T \sim \chi_{r-1}^2$.   

Jumping into actual calculations, we have $T = \frac{(18 - np_1)^2}{np_1} + \frac{(15 - np_2)^2}{np_2} + ... + \frac{(19 - np_6)^2}{np_6} =$ `r T_stat`, where $np_i = 15 \frac{2}{3}$, for $i = 1,...,6$. Under our $H_0$, our p-value is `r p_value`, not sufficient evidence to reject our $H_0$ and conclude that hurricane landfalls are different across decades, but perhaps a contentious talking point in conversation. Obviously the scope and depth of the analysis here isn't sufficient to add meaningfully to the overall discussion about climate change.   

*2. Contingency tables, testing for homogeneity and independence*

To demonstrate usage of contigency tables and associated tests using $R$, an article published by medical researchers that followed $N = 6272$, Swedish men for 30 years to see whether there was any association between the amount of fish in their diet and prostate cancer was located. According to authors of the research, the original study actually used pairs of twins. This approach allowed researchers to share their findings with more confidence. In the table below, we show frequency counts among research respondents across their fish diet type, and whether or not they contracted prostate cancer across the 30 year study:  

```{r cont_tables, include = T, echo =F}
cdat <- read.table(file = '/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/fish-diet.txt',header=T,stringsAsFactors = F)
cdat <- transform(cdat, Diet.Counts = factor(Diet.Counts, levels = c('Large','Moderate','Small','Never')))
dres <- table(cdat$Cancer.Counts,cdat$Diet.Counts) 
knitr::kable(dres, caption = 'Occurence of Prostate Cancer, By Amount of Fish Consumption, N = 6272')
```

*Source: Lancet, June 2001, “Fatty Fish Consumption and Risk of Prostate Cancer”*

The table itself does not communicate readily any differences in cancer rates by diet type. To evaluate further how diet type may impact the rate of cancer, we utilize the chi-square test of independence, which is readily available via the function *chisq.est* in base $R$. Given a contingency table of consisting of $m$ levels of a factor, along with an additional factor with $k$ levels of observations, we can construct a matrix $N = (n_{ij}), i = 1,...,m,\ j = 1,...,k$, where each entry in $N$ is an observed frequency, or count.  The main gist of the chi-square test, is to study observed frequencies in $N$, $n_{ij}$, and their potential differences from the expected (under the hypothesis of independence) frequencies, denoted: $\hat{n_{ij}} = \frac{n_{i.} \cdot n_{j.}}{n_{..}}$, where $n_{i.} = \sum_{j=1}^{k}n_{ij}$, and $n_{j.} = \sum_{i=1}^{m}n_{ij}$, and compute our test statistic $T = \sum_{i=1}^{m} \sum_{j=1}^{k} \frac{(n_{ij} - \hat{n_{ij}})^2}{\hat{n_{ij}}}$, where $T \sim \chi^2$. The results of the test for our fish diet dataset are summarized in the table below:   

```{r cs_test_res, include = T, echo = F}
cres <- chisq.test(dres)
cout <- data.frame(variable =c('Test Statistic (Chi)','P-value') , 
                   value = c(round(cres$statistic,digits = 3),round(cres$p.value,digits = 3)),stringsAsFactors = F)
knitr::kable(cout, caption = 'Chi-square Test for Independence Test, R (base), function: "chisq.test (df = 3)"', 
             col.names = c("Result","Value"), row.names = F)
```

As shown, our resulting P-value $0.298$ does not provide evidence for us to conclude that the observed frequency of prostate cancer by fish diet type, is unexpected by itself. While this finding may be interesting, this test alone isn't sufficient to rule out diet's role in respondent's contraction of prostate, as there are potentially many other factors which can contribute to prostate cancer in addition to diet alone. 

*3. Fisher exact test* 

Taking our analyis of prostate cancer, and fish diet further, visual inspection of the marginal proportion of prostate cancer by fish diet classification shows that those who never consumed fish in their diet, may have had slightly higher rates of prostate cancer, which was identified and estimated in the *Lancet* paper referenced above. Here is a basic barplot to visualize difference in rate of cancer by diet: 

```{r barplot_fisher, include = T, echo =F, fig.align='center',fig.height=4, fig.width=5}
vres <- table(cdat$Cancer.Counts,cdat$Diet.Counts) 
barplot(prop.table(vres,margin = 2), beside = T, horiz = T,
        main = 'Marginal Proportion of Respondents With Prostate Cancer by Fish Diet',
        cex.names = 0.73,cex.main = 0.7, xlim = c(0,1),cex.axis = 0.8) 
legend(0.7,12.7,c('Cancer = No','Cancer = Yes'), bty = 'n', fill = c('gray27','grey'),cex = 0.5)
```

Since, as shown in the text, the Fisher exact test is based on the null hypothesis that *two* factors, each with *two* factor levels, are independent, conditional on fixing marginal frequencies for both factors. However, the fish diet dataset described above is $2 \times 4$, therefore for the purposes of this question, we subset our data to just those with 'Never', and 'Large' fish diets for comparison. 

As shown below, relying on the only the *fisher.test* function to compute *Fisher's Exact Test* in *R (base)* to detect any differences in counts across diet categories. The test shown below, relies on the odds ratio between the occurence of cancer between groups ('Never' and 'Large'): 

```{r fisher_test_fish, include = T, echo = F}
fres <- fisher.test(vres[,c(1,4)])
fres_df <- data.frame(
  Result = c('Odds Ratio','Upper Confidence Level','Lower Confidence Level','P-Value'),
  Value = round(c(fres$estimate,fres$conf.int[1],fres$conf.int[2],fres$p.value),digits = 3)
)
knitr::kable(fres_df, caption = "Fisher's Exact Test: Fish Diet Comparison (Confidence = 0.95)")
```

Again, our resulting P-value from *Fisher's Exact Test* does not provide evidence for us to conclude that the observed frequency of prostate cancer by fish diet type, is unexpected. As highlighted before, there are potentially many other factors which can contribute to prostate cancer in addition to diet alone. The research referenced above in *Lancet* takes a deeper dive into this particular dataset and concludes that diet, in fact, did play a role describing respondents' rate of contracting prostate cancer, however, relying on more sophisticated statistical techniques. 

## 3. Nonparametric Regression (25%)


*Locate a data set suitable for both kernel and spline regressions. The data should include at least 3 x-variables.  It is okay to focus on additive-models discussed in lectures.*

To consider procedures for both kernel and spline regression, we utilize the well-known automobile MPG dataset that is available in the UCI Machine Learning library. Link here: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data. The dataset was used in the 1983 American Statistical Association Exposition, and is also well referenced in the following paper:

*Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst.*

As highlighted by Quinlan in the paper above, this dataset is concerned with city-cycle fuel consumption in miles per gallon, to be predicted in terms of a number of attributes of each car. Each car in the dataset ($N=392$) was manufactured sometime in between 1970 and 1982. For convenience purposes we have removed observations with missing values. Based on the exercise specification we will rely on a car's *Mileage Per Gallon* (mpg), as our outcome of interest, and utilize $3$ co-variates, namely the car's *Cylinder* count (cyl), *Engine Displacement* (disp), and *Acceleration* (accel). A preview of the data is provided below:     

```{r auto_reg, include = T, echo = F}
# adat <- read.table(file = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data", 
#                    header = F, stringsAsFactors = F)
# colnames(adat) <- c('mpg','cyl','disp','horse','wtg','accel','year','origin','name')
# adat <- data.table(adat)
# adat <- adat[horse != '?']
# adat <- transform(adat, horse = as.numeric(horse))
# saveRDS(adat, '/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/auto_mpg.rds')
adat <- readRDS('/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/auto_mpg.rds')
knitr::kable(head(adat[,c('mpg','disp','cyl','accel','name'),with=F]), col.names = c('MPG','Displacement','Cylinder Count','Acceleration','Model'), caption = 'Preview of Automobile MPG Dataset (Published 1993)')
```

One aspect of this dataset that makes it suited to kernel and spline regression, is that visually, there is evidence of non-linear relationship between the outcome of interest (*MPG*) and *Engine Displacement*. The relationship between *MPG*, *Cylinder* count and *Acceleration* is less clear, and shown below in a series of scatter plots:  

```{r auto_plot, include = T, echo= F, fig.align='center',fig.height=3}
par(mfrow=c(1,3))
with(adat, {
  plot(disp,mpg, bty= 'n',xlab = 'Engine Displacement', ylab = 'MPG',main = 'MPG vs. Displacement, Cylinder Count and Acceleration',cex.main = 0.67)
  plot(cyl,mpg,bty= 'n',xlab = 'Cylinder Count', ylab = '')
  plot(accel,mpg,bty= 'n',xlab = 'Acceleration', ylab = '')
})
```

*1)	Go through one-variable-at-a-time kernel- and spline-regression fits to the data for all 3 x-variables. Compare the fitting results using the leave-one-out cross-validation procedure.*  

Iterating over the auto-mpg set, we fit both a spline, and kernel regression model, removing one target observation, and compared the prediction error generated out of sample for each model. Summary statistics resulting from the leave-one-out computation are presented below:  

```{r leave_oo, include =T, echo = F}
#leave one out procedure - spline and kernel regression
# CPU distributed (10 cores here)
# spl_res <- rbindlist(mclapply(1:nrow(adat), function(i){
#   train <- adat[-i]
#   test <- adat[i]
#   #additive spline fits
#   spl_fit_x1 <- smooth.spline(train$disp,train$mpg, cv= F,df = 3)
#   spl_fit_x2 <- smooth.spline(train$cyl,residuals(spl_fit_x1), cv= F,df = 3)
#   spl_fit_x3 <- smooth.spline(train$accel,residuals(spl_fit_x2), cv= F,df = 3)
#   test <- transform(test, prediction = predict(spl_fit_x1, data.frame(test$disp))$y + 
#                       predict(spl_fit_x2, data.frame(test$cyl))$y + predict(spl_fit_x3, data.frame(test$accel))$y)
#   test$error <- test$mpg - test$prediction #actual vs. estimate
#   test
# }, mc.cores=10))
# 
# #this takes 1-2 minutes with 10 cores
# kr_res <- rbindlist(mclapply(1:nrow(adat), function(i){
#   train <- adat[-i]
#   test <- adat[i]
#   #additive kernel reg fits - simplified predict if data.frame is called
#   kr_fit_x1 <- npreg(mpg~disp,gradients = TRUE, data = train)
#   train$e1 <- residuals(kr_fit_x1)
#   kr_fit_x2 <- npreg(e1~cyl,gradients = TRUE, data = train)
#   train$e2 <- residuals(kr_fit_x2)
#   kr_fit_x3 <- npreg(e2~accel,gradients = TRUE, data = train)
#   test$prediction <- predict(kr_fit_x1, newdata = data.frame(disp = test$disp)) + 
#                       predict(kr_fit_x2, newdata = data.frame(cyl = test$cyl)) + 
#                       predict(kr_fit_x3, newdata = data.frame(accel = test$accel))
#   test$error <- test$mpg - test$prediction #actual vs. estimate
#   test
# }, mc.cores=10))
# 

# saveRDS(list(spline = spl_res, kernel = kr_res), file = '/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/loo_res.rds')
loo_res <- readRDS(file ='/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/loo_res.rds' )
#error only df
ldat <- data.table(
  error = c(loo_res$spline$error,loo_res$kernel$error),
  method = c(rep('Spline',nrow(adat)), rep('Kernel Regression',nrow(adat)))
)
loo_res <- rbindlist(loo_res,use.names = F)
loo_res$method <- c(rep('Spline',nrow(adat)),rep('Kernel Regression',nrow(adat)))
comp_rsq <- function(y, predy){
  1 - sum((y - predy)^2)/ sum((y - mean(y))^2)
}
lsum <- loo_res[,.(median_err = median(error), q75 = quantile(error, 0.75),q25 = quantile(error, 0.25), rsq = comp_rsq(y = mpg, predy = prediction.test.disp), mape = mean(abs( mpg/prediction.test.disp -1 ))*100),.(method)]
lsum <- melt.data.table(lsum, id.vars = 'method')
lsum <- dcast.data.table(lsum, variable ~ method)
lsum$variable <- c('Median Error','Error: 75th Percentile','Error: 25th Percentile','R-Squared','Mean Absolute Percentage Error' )
knitr::kable(lsum, caption = 'Regression Model Error Comparisons: Leave-One-Out Procedure', digits = 2, col.names = c('Statistic','Kernel','Spline'))
```

The summary statistics from this table clearly highlight that both models perform similarly across a number of simple error statistics. While the kernel regression approach may have a slight edge across a few statistics, in general, both models provided results with symmetric, generally unbiased out-of-sample prediction error for each leave-out observation. While the leave-one-out procedure doesn't highlight any significant differences in performance, further analysis and computation below does show some of the practical benefits of and limitations of each approach to solving the regression problem of predicting a car's *MPG* based on its characteristics. 

Note that computational details for the leave-one-out procedure are detailed in the code appendix. 

*2)	Select 2 sets of x-locations, e.g., (1st set: x1 = 3, x2 = 5, x3 = 2, where 3, 5, 2 are values within x-data range). These 2 sets of x-locations should be from a location close to the center of x-data-range and another location closer to the edge. Make predictions of Y at these x-data. Compare the predictions from kernel and spline methods, and also comment on the impact from x-data-locations.*  

To select $X$-locations, we took the median across all $X$ variables, i.e. *Displacement*, *Cylinder Count*, and *Acceleration* as one $X$-location, referred to as the "Center Point" here. We then also created an "Outside Point" which is based on the maximum across all $X$ variables, less $1$. That is, a point very close to the maximum across all co-variates. The table below summarizes the exact location of these points, and also provides associated predicted values from both kernel regression and spline models that were fit to the full auto-mpg dataset:   

```{r location_pred, include =F, echo = F}
#fit entire dataset
#evaluate but don't include due to printout from npreg
sfit_x1 <- smooth.spline(adat$disp,adat$mpg, cv= F,df = 3)
sfit_x2 <- smooth.spline(adat$cyl,residuals(sfit_x1), cv= F,df = 3)
sfit_x3 <- smooth.spline(adat$accel,residuals(sfit_x2), cv= F,df = 3)

#swap out original df
kdat <- adat
kfit_x1 <- npreg(mpg~disp,gradients = TRUE, data = kdat)
kdat$e1 <- residuals(kfit_x1)
kfit_x2 <- npreg(e1~cyl,gradients = TRUE, data = kdat)
kdat$e2 <- residuals(kfit_x2)
kfit_x3 <- npreg(e2~accel,gradients = TRUE, data = kdat)
```

```{r loc_pred, include=T, echo = F}
#points for usage
cpt <- data.table(disp = median(adat$disp),cyl = median(adat$cyl), accel = median(adat$accel), type = 'center point')
opt <- data.table(disp = max(adat$disp) - 1,cyl = max(adat$cyl)-1, accel = max(adat$accel) - 1, type = 'outside point')

cpt$kr_pred <- round(predict(kfit_x1, newdata = data.frame(disp = cpt$disp)) + 
                      predict(kfit_x2, newdata = data.frame(cyl = cpt$cyl)) +
                      predict(kfit_x3, newdata = data.frame(accel = cpt$accel)),digits = 2)
opt$kr_pred <- round(predict(kfit_x1, newdata = data.frame(disp = opt$disp)) + 
                      predict(kfit_x2, newdata = data.frame(cyl = opt$cyl)) +
                      predict(kfit_x3, newdata = data.frame(accel = opt$accel)),digits = 2)
cpt$spl_pred <- round(as.numeric(predict(sfit_x1, data.frame(cpt$disp))$y +
                       predict(sfit_x2, data.frame(cpt$cyl))$y + predict(sfit_x3, data.frame(cpt$accel))$y),digits = 2)
opt$spl_pred <- round(as.numeric(predict(sfit_x1, data.frame(opt$disp))$y +
                       predict(sfit_x2, data.frame(opt$cyl))$y + predict(sfit_x3, data.frame(opt$accel))$y),digits = 2)
pdat <- rbindlist(list(cpt,opt))
knitr::kable(pdat, caption = 'Comparison of Predictions from Spline & Kernel Regression Methods',
             col.names = c('Displacement','Cylinder','Acceleration','X-Location','Kernel Prediction','Spline Prediction'))
```
  
It is clear based on the table above that the spline provides a more conservative prediction of car MPG for both the center and outside points. To get a sense of how these predictions fare relative to our actual data, the scatter plot below shows our key predictor *Displacement* on the x-axis, against our outcome of interest *MPG* on the y-axis. The predicted MPG given each model is then shown in the colored points below:  

```{r kr_pred_plot, echo = F, include = T, fig.align='center',fig.height=4}
with(adat,{ 
     plot(disp,mpg, bty = 'n',xlab='Displacement',ylab='MPG',main = 'Center & Outer X-Location Predictions vs. Data',cex.main = 0.8)
    points(x = pdat$disp[1], y = pdat$kr_pred[1],  bg = 'red', pch = 23, cex = 1.5)
    points(x = pdat$disp[2], y = pdat$kr_pred[2],  bg = 'red', pch = 23, cex = 1.5)
    points(x = pdat$disp[1], y = pdat$spl_pred[1],  bg = 'blue', pch = 21, cex = 1.5)
    points(x = pdat$disp[2], y = pdat$spl_pred[2],  bg = 'blue', pch = 21, cex = 1.5)
    legend(300,40,c('Spline','Kernel Regression'),pch = c(21,23) ,bty = 'n',pt.bg = c('blue','red'), cex = 0.8)
  })
```

The scatter plot again shows that the spline provides a slightly more conservative prediction vs. kernel regression. However, both predictions lie in a reasonable location relative to actual data points observed. 

*3)	Construct the 90% Bias-Corrected Bootstrap CIs for the predictions at the selected 2-sets of x-locations. Show details of the bias-correction process. Compare the CIs at two x-locations, and also from two nonparametric regression methods.*  

Continuing our analysis of prediction at our two $X$-locations, we conduct a bootstrap sampling analysis. The procedure conducted consists of:

1. Taking a boostrap sample, or a uniform sample of observations (selected with replacement)
2. Fitting a spline or kernel regression model to the boostrap sample data, and 
3. Predicting *MPG* at the given $X$-locations using the models fit in step 2

To create a more robust bootstrap estimate, we use bias-correction, or *BCa*, a technique similar to a percentile approach to boostrap sampling. However the bootstrap percentiles are adjusted to account for bias and skewness. Underlying the the *BCa* approach is a notion of acceleration and skewness informing the parameter intervals, and also a connection to the normal distribution, as *BCa* relies on the CDF of a standard normal random variable to compute percentile estimates. The results of the bootstrap analysis are summarized in the table below:  

```{r bca_boot, include =T, echo = F}
#read in results from a separate script --
boot_res <- readRDS('/Users/peterwilliams/Projects/pewilliams-isye-6404/exams/exam3/data/boot_res.rds')

knitr::kable(boot_res, caption = 'Bias-Corrected Bootstrap CIs - 90%, By Model Type and X-location',
             colnames = c('X-Location','Prediction','Lower CI','Upper CI','Model'),digits = 2)
```

*Note: Instability of the kernel regression fits from the np package, and instability of fits from smooth.spline in R (base) were both problematic due to sample size, and computation speed constraints. Further analysis would have likely yielded more stable results.*

A key point to highlight from this table, is the instability of the kernel regression fits given our dataset. Bootstrap replication actually created non-informative confidence intervals for our kernel regression model, given poor fits of in some boostrap replications. In contrast results from the spine model were more informative, and much narrower, and perhaps precise in comparison to kernel regression. The stability of the spline model in comparison does highlight its utility in this very specific case. 

The bootstrap computational details were somewhat lengthy relative to other routines, and are provided in the *Code Appendix* at the end of this document. 

\newpage

## Code Appendix

#### Multi-resolution Plot Generation Example (R package: *waveslim*)

The code below demonstrates procedures to plot the additive components of a wavelet decomposition using the *waveslim* package in $R$. Note this code depends on usage of a time series exactly $1024$ observations in length:   

```{r append_mra, echo = T, eval =F }
x <- as.numeric(aep$AEP_MW) #input megawatts time series

n.levels <- 4
aep.haar <- mra(x = x, "haar", n.levels, "dwt")
nplots <- n.levels + 2
par(mfcol=c(nplots,2), pty="m", mar=c(5-2.5,4,4-2,1.5), 
    cex.main = 0.9)
plot.ts(x, axes=F, main="Haar Filter: Multi-Resolution Plot", 
        ylab = 'Original', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(x),digits = -4),round(max(x),digits = -4)), 
     labels=c(round(min(x),digits = -4),round(max(x),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(aep.haar[[i]], axes=F, ylab=names(aep.haar)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(aep.haar[[i]]),digits = -2),
                    round(max(aep.haar[[i]]),digits = -2)), 
       labels=c(round(min(aep.haar[[i]]),digits = -2),
                round(max(aep.haar[[i]]),digits = -2)))
}

#family daubechies
aep.la8 <- mra(x = x, "la8", n.levels, "dwt")
plot.ts(x, axes=F, main="Daubechies Filter (L=8): Multi-Resolution Plot", 
        ylab = 'Original', bty='n')
axis(side=1, at=seq(0,1024,by=128), 
     labels=seq(0,1024,by=128))
axis(side=2, at=c(round(min(x),digits = -4),round(max(x),digits = -4)), 
     labels=c(round(min(x),digits = -4),round(max(x),digits = -4)))
for(i in 1:(n.levels +1)){
  plot.ts(aep.la8[[i]], axes=F, ylab=names(aep.la8)[i],bty='n')
  axis(side=1, at=seq(0,1024,by=128), 
       labels=seq(0,1024,by=128))
  axis(side=2, at=c(round(min(aep.la8[[i]]),digits = -2),
                    round(max(aep.la8[[i]]),digits = -2)), 
       labels=c(round(min(aep.la8[[i]]),digits = -2),
                round(max(aep.la8[[i]]),digits = -2)))
}
```
  
#### Leave-One-Out Procedure, Kernel and Spline Regression

Below is exemplary code utilized in this report to generate additive model fits for both spline, and kernel regression models, along with the leave-one-out procedure. To speed up computation, which can be slow for kernel regression, the routine is distributed across CPUs utilizing the *parallel* library in $R$:

```{r append_loo, echo = T, eval =F }
#leave one out procedure - spline and kernel regression
# CPU distributed (10 cores here)
spl_res <- rbindlist(mclapply(1:nrow(adat), function(i){
  train <- adat[-i]
  test <- adat[i]
  #additive spline fits
  spl_fit_x1 <- smooth.spline(train$disp,train$mpg, cv= F,df = 3)
  spl_fit_x2 <- smooth.spline(train$cyl,residuals(spl_fit_x1), cv= F,df = 3)
  spl_fit_x3 <- smooth.spline(train$accel,residuals(spl_fit_x2), cv= F,df = 3)
  test <- transform(test, 
                    prediction = predict(spl_fit_x1, data.frame(test$disp))$y + 
                      predict(spl_fit_x2, data.frame(test$cyl))$y + 
                      predict(spl_fit_x3, data.frame(test$accel))$y)
  test$error <- test$mpg - test$prediction #actual vs. estimate
  test
}, mc.cores=10))

#this takes 1-2 minutes with 10 cores
kr_res <- rbindlist(mclapply(1:nrow(adat), function(i){
  train <- adat[-i]
  test <- adat[i]
  #additive kernel reg fits - simplified predict if data.frame is called
  kr_fit_x1 <- npreg(mpg~disp,gradients = TRUE, data = train)
  train$e1 <- residuals(kr_fit_x1)
  kr_fit_x2 <- npreg(e1~cyl,gradients = TRUE, data = train)
  train$e2 <- residuals(kr_fit_x2)
  kr_fit_x3 <- npreg(e2~accel,gradients = TRUE, data = train)
  test$prediction <- predict(kr_fit_x1, newdata = data.frame(disp = test$disp)) + 
                      predict(kr_fit_x2, newdata = data.frame(cyl = test$cyl)) + 
                      predict(kr_fit_x3, newdata = data.frame(accel = test$accel))
  test$error <- test$mpg - test$prediction #actual vs. estimate
  test
}, mc.cores=10))
```

#### Bootstrap Confidence Intervals

Below is exemplary code utilized in this report to generate boostrap confidence intervals using the boot package, relying on the *boot* and *np* packages. Bootstrap sampling procedures were CPU distributed where possible to speed up computation:   

```{r append_boot, eval = F, echo =T}
B <- 10000 #bootstrap replication count
#estimator function
adat <- readRDS('data/auto_mpg.rds')
cpt <- data.table(disp = median(adat$disp),cyl = median(adat$cyl), 
                  accel = median(adat$accel), type = 'center point')
opt <- data.table(disp = max(adat$disp) - 1,cyl = max(adat$cyl)-1, 
                  accel = max(adat$accel) - 1, type = 'outside point')
spl_boot_cpt <- function(data, indices){
  #indices <- sample(1:nrow(adat),replace = T)
  bs_dat <- data[indices]
  sfit_x1 <- smooth.spline(bs_dat$disp,bs_dat$mpg, cv= F,df = 3)
  sfit_x2 <- smooth.spline(bs_dat$cyl,residuals(sfit_x1), cv= F,df = 3)
  sfit_x3 <- smooth.spline(bs_dat$accel,residuals(sfit_x2), cv= F,df = 3)
  prediction <- as.numeric(predict(sfit_x1, data.frame(cpt$disp))$y +
                             predict(sfit_x2, data.frame(cpt$cyl))$y + 
                             predict(sfit_x3, data.frame(cpt$accel))$y)
  prediction
}

spl_boot_opt <- function(data, indices){
  #indices <- sample(1:nrow(adat),replace = T)
  bs_dat <- data[indices]
  sfit_x1 <- smooth.spline(bs_dat$disp,bs_dat$mpg, cv= F,df = 3)
  sfit_x2 <- smooth.spline(bs_dat$cyl,residuals(sfit_x1), cv= F,df = 3)
  sfit_x3 <- smooth.spline(bs_dat$accel,residuals(sfit_x2), cv= F,df = 3)
  prediction <- as.numeric(predict(sfit_x1, data.frame(opt$disp))$y +
                             predict(sfit_x2, data.frame(opt$cyl))$y + 
                             predict(sfit_x3, data.frame(opt$accel))$y)
  prediction
}

spl_cpt_res <- boot(adat, statistic = spl_boot_cpt,
                    R=B)
spl_opt_res <- boot(adat, statistic = spl_boot_opt,
                    R=B)
#bootstrap results - spline
cpt_results <- boot.ci(boot.out = spl_cpt_res,type = 'bca',conf = 0.9)
opt_results <- boot.ci(boot.out = spl_opt_res,type = 'bca',conf = 0.9)

boot_spl_bca <- data.table(
  Point = c('Center Point','Outside Point'),
  Prediction = c(cpt_results[[2]],opt_results[[2]]),
  Lower = c(cpt_results$bca[4],opt_results$bca[4]),
  Upper = c(cpt_results$bca[5],opt_results$bca[5]))



kr_boot_opt <- function(data, indices){
  #indices <- sample(1:nrow(adat),replace = T)
  kdat <- data[indices]
  kfit_x1 <- npreg(mpg~disp,gradients = TRUE, data = kdat)
  kdat$e1 <- residuals(kfit_x1)
  kfit_x2 <- npreg(e1~cyl,gradients = TRUE, data = kdat)
  kdat$e2 <- residuals(kfit_x2)
  kfit_x3 <- npreg(e2~accel,gradients = TRUE, data = kdat)
  prediction <- predict(kfit_x1, newdata = data.frame(disp = opt$disp)) + 
    predict(kfit_x2, newdata = data.frame(cyl = opt$cyl)) +
    predict(kfit_x3, newdata = data.frame(accel = opt$accel))
  prediction
}

kr_boot_cpt <- function(data, indices){
  #indices <- sample(1:nrow(adat),replace = T)
  kdat <- data[indices]
  kfit_x1 <- npreg(mpg~disp,gradients = TRUE, data = kdat)
  kdat$e1 <- residuals(kfit_x1)
  kfit_x2 <- npreg(e1~cyl,gradients = TRUE, data = kdat)
  kdat$e2 <- residuals(kfit_x2)
  kfit_x3 <- npreg(e2~accel,gradients = TRUE, data = kdat)
  prediction <- predict(kfit_x1, newdata = data.frame(disp = cpt$disp)) + 
    predict(kfit_x2, newdata = data.frame(cyl = cpt$cyl)) +
    predict(kfit_x3, newdata = data.frame(accel = cpt$accel))
  prediction
}

kr_cpt_res <- boot(adat, statistic = kr_boot_cpt,
                   R=B,parallel = 'multicore',
                   ncpus = 11)
kr_opt_res <- boot(adat, statistic = kr_boot_opt,
                   R=B,parallel = 'multicore',
                   ncpus = 11)

#bootstrap results - kernel regression
cpt_kresults <- boot.ci(boot.out = kr_cpt_res,type = 'bca',conf = 0.9)
opt_kresults <- boot.ci(boot.out = kr_opt_res,type = 'bca',conf = 0.9)

#results summary
boot_kr_bca <- data.table(
  Point = c('Center Point','Outside Point'),
  Prediction = c(cpt_kresults[[2]],opt_kresults[[2]]),
  Lower = c(cpt_kresults$bca[4],opt_kresults$bca[4]),
  Upper = c(cpt_kresults$bca[5],opt_kresults$bca[5]))

boot_kr_bca$Method <- 'Kernel Regression'
boot_spl_bca$Method <- 'Spline'
boot_sum <- rbindlist(list(boot_kr_bca,boot_spl_bca))
```